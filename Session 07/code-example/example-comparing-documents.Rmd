---
title: "Comparing documents"
output: html_document
---

# Distance metrics

```{r, include = F}
require(quanteda)
library(readr)
```

`textstat_simil()` computes matrices of distances and similarities between documents. It is useful for comparing the feature use across different documents.

Euclidean distance:

```{r}
docs <- c("this is document one", "this is document two")
(doc_dfm <- dfm(docs))
textstat_dist(doc_dfm, method="euclidean")
# let's do the math...
(d1 <- as.numeric(doc_dfm[1,]))
(d2 <- as.numeric(doc_dfm[2,]))
sqrt(sum((d1 - d2)^2))
```

Cosine similarity:

```{r}
textstat_simil(doc_dfm, method="cosine")
# some more math...
sum(d1 * d2) / ( sqrt(sum(d1^2)) *  sqrt(sum(d2^2)) )
```

Note that these two metrics measure the opposite thing: Euclidean distance measures how *different* documents are, whereas cosine similarity measures how *similar* documents are. Of course, it's easy to reverse them; generally, we can just say (1 - distance) = similarity.

Edit distance:

```{r}
textstat_simil(doc_dfm, method="hamman")
```


And here's an example of how we would apply these metrics in practice. Let's say I want to build a recommendation engine for my favorite type of movie.

```{r}
# data source: http://www.cs.cmu.edu/~ark/personas/
movie <- readr::read_csv(unz("../../data/movie-plots.csv.zip", "movie-plots.csv"), col_types="cccc")
# now we find a movie i like
scifi <- which(movie$name=="Gravity")
movie[scifi,]
# pre-process the data
mcorp <- corpus(movie, text_field = "plot")
docnames(mcorp) <- docvars(mcorp)$name
mdfm <- dfm(mcorp, verbose=TRUE, remove_punct=TRUE,
            remove_numbers=TRUE, remove=stopwords("english"))
# and I will compute cosine similarity with respect to all other movies
simil <- textstat_simil(dfm_tfidf(mdfm), selection = scifi, method="cosine")
simil <- simil[order(simil, decreasing=TRUE),]
head(simil, n=5)
# and we can read their plots
movie$plot[movie$name %in% names(simil)[2:5]]
```

# Clustering methods

First we will explore an application of k-means clustering to the plots of recent movies:

```{r}
recent <- corpus_subset(mcorp, release_year>=2010)
mdfm <- dfm(recent, verbose=TRUE, remove_punct=TRUE,
            remove_numbers=TRUE, remove=stopwords("english"))
cdfm <- dfm_weight(dfm_trim(mdfm, min_docfreq = 5, verbose=TRUE), "prop")
set.seed(777) # set random seed to ensure replicability
kc <- kmeans(cdfm, centers=5)
table(kc$cluster)
head(docvars(recent)$name[kc$cluster==1])
head(docvars(recent)$name[kc$cluster==2])
head(docvars(recent)$name[kc$cluster==3])
head(docvars(recent)$name[kc$cluster==4])
head(docvars(recent)$name[kc$cluster==5])
# action movies?
head(textstat_keyness(cdfm, target=kc$cluster==1),n=20)
# romantic movies?
head(textstat_keyness(cdfm, target=kc$cluster==2),n=20)
# independent films?
head(textstat_keyness(cdfm, target=kc$cluster==3),n=20)
# drama?
head(textstat_keyness(cdfm, target=kc$cluster==4),n=20)
# comedy?
head(textstat_keyness(cdfm, target=kc$cluster==5),n=20)
```

Hierarchical clustering is an alternative approach to group documents. It relies on the matrix of distances between documents and works from the bottom up to create clusters: starting with lowest pairwise distance, then sequentially merges documents into clusters as the distances become larger.

```{r}
library(quanteda.corpora)
pres_dfm <- dfm(corpus_subset(data_corpus_sotu, Date > "1980-01-01"), 
               stem = TRUE, remove_punct = TRUE,
               remove = stopwords("english"))
pres_dfm <- dfm_weight(dfm_trim(pres_dfm, min_termfreq = 5, min_docfreq = 3), "prop")
# hierarchical clustering - get distances on normalized dfm
pres_dist_mat <- textstat_dist(pres_dfm, method = "euclidean")
# hiarchical clustering the distance object
pres_cluster <- hclust(as.dist(pres_dist_mat))
# label with document names
pres_cluster$labels <- docnames(pres_dfm)

plot(pres_cluster)
```
