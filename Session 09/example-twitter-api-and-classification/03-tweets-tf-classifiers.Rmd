---
title: "LASSO and random forest classifiers based on term frequencies"
output: html_document
---

Loading packages:

```{r}
library("tidyverse")
library("quanteda")
library("glmnet")
library("ranger")
library("tm")
```

### 1. Classifier specific pre-processing

Loading the data:

```{r}
df <- read.csv("../../data/labeled_tweets_processed.csv")
```

Creating a training / test split which is equal for all classifiers:

```{r}
set.seed(24)
test_fraction <- 0.3
training_indices <- sample(1:nrow(df), floor(nrow(df)*(1-test_fraction)))
```

Creating a dfm:

```{r}
dfm_tweets <- df$text %>% corpus %>%
  tokens() %>%
  tokens_remove(stopwords("en"), padding = TRUE) %>%
  tokens_ngrams(n = 1:2) %>% # up to bigrams
  dfm() %>%
  dfm_trim(min_termfreq = 5) %>% 
  dfm_weight(scheme = "prop")
# Sorting the columns as the order might change when rerunnning
dfm_tweets <- dfm_tweets[,sort(featnames(dfm_tweets))]
dfm_tweets
rowSums(dfm_tweets)[1:10]
```

Training and test sets:

```{r}
training_X <- dfm_tweets[training_indices,]
training_y <- factor(df[training_indices, "label"])
test_X <- dfm_tweets[-training_indices,]
test_y <- factor(df[-training_indices, "label"])
```

### 2. Multinomial LASSO classifier

#### 2.1 Training

Depending on the dataset size, this can run some time:

```{r}
lasso_model <- cv.glmnet(training_X, training_y, 
    family="multinomial", alpha=1, nfolds=5, intercept=TRUE, standardize = TRUE)
```

#### 2.2 Evaluation

```{r}
# Prediction
test_y_hat <- predict(lasso_model, test_X, type="class")
# Accuracy
sum(test_y_hat == test_y)/length(test_y)
# Confusion matrix
table(test_y_hat, test_y)
```


### 3. Excursus: Training a random forest

Random forests are very robust classifiers also for basic text classification. The `ranger` package offers a faster implementation of random forests in R. Otherwise the `randomForest` package is the most commonly used one. For an excellent introduction to trees and random forests, read the relatively short Chapter 8 in James et al. (2013) which is freely available to download [here](https://www.statlearning.com/).

#### 3.1 Training

Not that for large datasets, this can take considerable time:

```{r}
rf_model <- ranger(x = training_X, y = training_y, importance = "impurity")
```

#### 3.2 Evaluation

```{r}
# Prediction
test_y_hat <- predict(rf_model, test_X)$predictions
# Accuracy
sum(test_y_hat == test_y)/length(test_y)
# Confusion matrix
table(test_y_hat, test_y)
```

#### 3.3 Feature importance

Random forests come with an own variable importance measure. In a nutshell, it provides a measure of the average improvement in classifications achieved at splits in the threes which are based on the respective variable:

```{r}
importance(rf_model) %>% sort(decreasing = TRUE) %>% head(20)
```


### 4. Generic function to predict approval and disapproval for sentences

The last question is, how we can write a function which takes a generic sentence as input and estimates whether it is approving, disapproving, or neutral:

```{r}
approve_or_dissapprove <- function(sentence, model, model_dfm = dfm_tweets) {
  
  # Cleaning text before passing it through the dfm matrix
  
  # Convert to lower case
  sentence <- sentence %>% tolower()
  
  # Remove Twitter handles and hashtags
  sentence <- str_replace_all(sentence, pattern = "[@#]\\S+", replacement = "")
  
  # Remove URLs
  sentence <- str_replace_all(sentence, pattern = "(http|www)\\S+", replacement = "")
  
  # Get rid of non ASCII chracters (largely emojis in this case)
  sentence <-gsub("[^\x01-\x7F]", "", sentence)
  
  # Remove punctuation, numbers, and excess white spaces within the texts and at their beginning/end
  sentence <- sentence %>% removePunctuation() %>%
    removeNumbers() %>% stripWhitespace() %>% trimws()
  
  # Creating a dfm from the sentence
  dfm_sentence <- sentence %>% corpus %>% tokens() %>%
  tokens_remove(stopwords("en"), padding = TRUE) %>%
  tokens_ngrams(n = 1:2) %>% # up to bigrams
  dfm() %>% dfm_weight(scheme = "prop")
  
  # Matching the feature of this dfm with the features of the model dfm
  dfm_sentence <- dfm_match(dfm_sentence, featnames(model_dfm))  
  # Making a prediction
  if (class(model) == "cv.glmnet") {
    
    predicted_label <- predict(model, dfm_sentence, type = "class") %>% as.character()
  
  } else if (class(model) == "ranger") {
    
    predicted_label <- predict(model, dfm_sentence)$predictions %>% as.character()
    
  }
  
  
  return(predicted_label)
  
}
```

This function can now be tried out with some sample sentences. With the LASSO model:

```{r}
# Examples that work well
# Neutral
approve_or_dissapprove("This is a course in textual analysis.", model = lasso_model)
approve_or_dissapprove("The last week of lent term begins.", model = lasso_model)
# Approval
approve_or_dissapprove("I think this is a good policy.", model = lasso_model)
approve_or_dissapprove("The government is doing a great job.", model = lasso_model)
# Disapproval
approve_or_dissapprove("I think this is a bad policy.", model = lasso_model)
approve_or_dissapprove("The government is doing a bad job.", model = lasso_model)
# Examples of what it misses:
approve_or_dissapprove("I dont think this is a good policy.", model = lasso_model)
approve_or_dissapprove("I do not think this is a good policy.", model = lasso_model)
approve_or_dissapprove("I think this is an valuable policy.", model = lasso_model)
```

With the random forest:

```{r}
# Examples that work well
# Neutral
approve_or_dissapprove("This is a course in textual analysis.", model = rf_model)
approve_or_dissapprove("The last week of lent term begins.", model = rf_model)
# Approval
approve_or_dissapprove("I think this is a good policy.", model = rf_model)
approve_or_dissapprove("The government is doing a great job.", model = rf_model)
# Disapproval
approve_or_dissapprove("I think this is a bad policy.", model = rf_model)
approve_or_dissapprove("The government is doing a bad job.", model = rf_model)
approve_or_dissapprove("I dont think this is a good policy.", model = rf_model)
# Examples that it misses
approve_or_dissapprove("I do not think this is a good policy.", model = rf_model)
approve_or_dissapprove("I think this is an valuable policy.", model = rf_model)
```

Note that the classifiers are trained on only

```{r}
nrow(training_X)
```

tweets in this illustration. As tweets are very noisy data, a larger set of observations would be much preferable to obtain a better fit, however, of course increase training times.

### 5. Extensions

Note how widely the logic of these approaches is applicable. Imagine the following classifier:

```{r}
covid_emojis <- "\U0001F637,\U0001F9A0,\U0001F489"
cat(covid_emojis)
```

Neutral: General sample of tweets

General covid: Medical mask and microbe emojis

Vaccine: Syringe emoji

Such a classifier could then be used to separate discussions about general covid from discussions about vaccines automatically prior to a subsequent analysis..

Another option could be to create a binary classifier which predicts emojis such as

```{r}
cat(c("\U0001F621", "\U0001F92C", "\U0001F624", "\U0001F620"))
```

vs. a random sample of other texts.

This could then e.g. be used to aide the detection of toxic content and potentially moderate it more effectively.

