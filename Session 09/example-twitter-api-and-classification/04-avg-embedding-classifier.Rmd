---
title: "LASSO classifier based on average word embeddings"
output: html_document
---

This short notebook discusses an alternative method which uses average word embeddings as inputs in a LASSO classifier to predict the label of short texts. This can be an interesting approach, however, it does not seem to improve things in our example here. In general it can be worth a try in classification of very short texts (see e.g. the file `word-embeddings-application.html` by Pablo Barber√° in the materials contained in week 10 of the course repo.).

Loading packages:

```{r setup}
library("tidyverse")
library("quanteda")
library("glmnet")
library("ranger")
library("tm")

fread_zip = function(fp, silent=FALSE){
  qfp = shQuote(fp)

  patt = "unzip -cq %s"

  thecall = sprintf(patt, qfp)
  if (!silent) cat("The call:", thecall, sep="\n")

  data.table::fread(thecall, data.table = FALSE,  encoding = 'UTF-8')
}
```

Loading the data:

```{r}
df <- read.csv("../../data/labeled_tweets_processed.csv")
```

Loading GloVe word embeddings:

```{r}
# download as zip data from https://www.kaggle.com/thanakomsn/glove6b300dtxt
embeddings = fread_zip("../../data/glove.6B.300d.zip")
colnames(embeddings) = c('word', paste('dim',1:300,sep = '_'))
rownames(embeddings) <- embeddings$word
embeddings$word <- NULL
```

Creating a training / test split which is equal for all classifiers:

```{r}
set.seed(24)
test_fraction <- 0.3
training_indices <- sample(1:nrow(df), floor(nrow(df)*(1-test_fraction)))
```

Creating a dfm (we impose more regularisation with `min_termfreq = 20` as the lasso cannot just drop the unimportant words here):

```{r}
dfm_tweets <- df$text %>% corpus %>%
  tokens() %>%
  tokens_remove(stopwords("en")) %>%
  dfm() %>%
  dfm_trim(min_termfreq = 20)
sum(rowSums(dfm_tweets)==0)
```

Keeping only the word vectors relevant here:

```{r}
embeddings <- embeddings[featnames(dfm_tweets), ]
```


Now the trick is the following: Instead of using a row vector from the dfm to predict the label of a document, we will use its average embedding. Thus, we obtain the embeddings for all words in the document and take the average to predict the label. The model will therefore be based on 300 features here instead of the almost 10,000 considered before. Each feature does not represent a word anymore, but a dimension from the embedding space.

Storing the average embedding vector for each tweet/document in a matrix called `X` (running this can take a while depending on the size of the dataset):

```{r}
X <- matrix(0, nrow = nrow(dfm_tweets), ncol = 300)
# Loop over all tweets/documents
for (i in 1:ndoc(dfm_tweets)){
  # Status update every one thousands processed documents
  if (i %% 1000 == 0) message("Processing document ", i, " of ", ndoc(dfm_tweets), ".")
  
  # Get word counts of current document
  word_counts <- as.numeric(dfm_tweets[i,])
  
  # Store words in character vector which have counts > 0 in sentence
  words_in_document <- featnames(dfm_tweets)[word_counts>0]
  
  # Get embeddings of these words
  embeddings_in_sentence <- embeddings[words_in_document,]
  
  # Create document level embedding by taking mean
  X[i,] <- colMeans(embeddings_in_sentence, na.rm=TRUE)
  
  # If no word in sentence was found in glove matrix, set vector to zero
  if (any(is.na(X[i,]))) {X[i,] <- 0}
}
```

Training and test sets:

```{r}
training_X <- X[training_indices,]
training_y <- factor(df[training_indices, "label"])
test_X <- X[-training_indices,]
test_y <- factor(df[-training_indices, "label"])
```

Training a LASSO classifier (this also runs a while):

```{r}
embedding_lasso_model <- cv.glmnet(training_X, training_y, 
    family="multinomial", alpha=1, nfolds=5, intercept=TRUE, standardize = TRUE)
```

Evaluation:

```{r}
# Prediction
test_y_hat <- predict(embedding_lasso_model, test_X, type="class")
# Accuracy
sum(test_y_hat == test_y)/length(test_y)

# Confusion matrix
table(test_y_hat, test_y)

```
