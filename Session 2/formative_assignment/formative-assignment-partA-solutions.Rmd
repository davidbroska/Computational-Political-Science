---
title: 'CPS Formative Assignment Part A \n Solutions'
subtitle: 'Data exploration'
author: "YOUR NAME HERE"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##"
)
```

### Intro

The purpose of this assignment is to familiarize yourself with R, RMarkdown, and `quanteda`. Specifically, we will look at functions that allow you to explore text data.

As with the rest of assignments throughout the term, the goal is to write R code in the chunks within this RMarkdown file that will accomplish the tasks detailed below. Where questions are asked, give your answer where indicated or in the grey box after the question.

Start by loading the `quanteda` package.

```{r, include = F}
library("quanteda")
```

You will use the corpus of inaugural addresses by U.S. presidents `data_corpus_inaugural` for this part of the exercise. These data are available as soon as you load `quanteda` and are lazily-loaded once you access them.



### Question 1 (3 points)

Here is code to inspect which inaugural address was the longest.

```{r}
which.max(nchar(texts(data_corpus_inaugural)))
```

Explain briefly how this answer works.

**Your answer here**

>Given the corpus object `data_corpus_inaugural` the `texts()` function returns a named character vector where one element represents one of the 58 inaugural speeches. For each of those speeches `nchar()` then counts the number of characters. For the resulting numeric vector, the function `which.max()` gives the index of the maximum count of characters across speeches. It turns out that the longest speech is at position 14 of the vector. Since the element names are preserved throughout this process we can see that William Henry Harrison held the longest speech back in 1841.

>The functions provided by quanteda allow to count the number of tokens (`ntoken()`), features (`nfeat()`), or syllables (`nsyllable()`), for example. It may be a detail, but `nchar()` counts every single character whereas these quanteda functions count groups of characters. Nevertheless, we obtain the same result as with the code based on `nchar()`.

>Note: this answer is way more extensive than it is required for obtaining the full three points!




How could you do this using a **quanteda** function? You might want to a have look at `ntoken()` from the quanteda package and `sort()` from base R. Use `?` to interrogate what a function does, e.g. `?ntoken`.

```{r}
# possible answers include

ntoken(data_corpus_inaugural) %>%
  sort(decreasing = TRUE) %>%
  head(n = 1)

# What is %>%? It is called the pipe used to perform sequential tasks. For example:
# I woke up %>% showered %>% dressed %>% glammed up %>% took breakfast %>% showed up to work 

# https://twitter.com/WeAreRLadies/status/1172576445794803713?s=20
```



### Question 2 (3 points)

Generate a document-feature matrix from this corpus and call it `sotu_dfm`. Make sure that the text is put into __lowercase__ and words are __stemmed__ as preprocessing steps.

```{r}
sotu_dfm <- dfm(data_corpus_inaugural, tolower = TRUE, stem = TRUE)
```

Now, look at the 20 most common features in the document-feature matrix. Hint: `?topfeatures`

```{r}
# examine top 20 common features
topfeatures(sotu_dfm, n = 20)
```

What do most of those tokens have in common? What step(s) might you take to focus on a more interesting set of word frequencies?

**Your answer here**

>All of them are function words or punctuation. Words like that can be considered stopwords and are often excluded from bag-of-words analyses.




### Question 3 (2 points)

Now generate a second document-feature matrix but remove punctuation and remove stopwords. Call this object `sotu_dfm_sp`.

```{r}
sotu_dfm_sp <- dfm(data_corpus_inaugural, tolower = TRUE, stem = TRUE, remove = stopwords("english"), remove_punct = TRUE)
```

Now look at the 20 most common features again. The most common words are now no longer uninteresting words.

```{r}
topfeatures(sotu_dfm_sp, n = 20)
```

To go further, we could also have used the `remove_numbers = TRUE` argument. Note that in **quanteda**, it is possible to tokenize the corpus first, using `tokens()`, and then create the dfm from those tokens. This provides an additional amount of control, through the many functions available in the [`tokens_*()` functions](http://docs.quanteda.io/reference/#section-tokens-functions).






### Question 4 (1 point)

Write code to figure out how many different types of punctuation and how many different types of stopwords were removed.

```{r}
ncol(sotu_dfm) - ncol(sotu_dfm_sp)
```



### Question 5 (3 points)

Plot a wordcloud of the dfm that has the stopwords and punctuation characters, and numbers removed, but without stemming. Plot this only for words with a minimum count of 10. Hint: Use `textplot_wordcloud()` to generate the plot.

```{r, warning=FALSE}
# Generate dfm
sotu_dfm_spn <- dfm(data_corpus_inaugural, tolower = TRUE, stem = FALSE, remove = stopwords("english"), remove_punct = TRUE, remove_numbers = TRUE)
# Remove words (types) with less than 10 occurrences
sotu_dfm_spn <- dfm_trim(sotu_dfm_spn, min_termfreq = 10, verbose = TRUE)
set.seed(123)
textplot_wordcloud(sotu_dfm_spn, min_size=.2, max_size=2, max_words=60)
```

### Question 6 (2 points)

Write code to search for the word "division" from the `data_corpus_inaugural` object using `kwic()`.

```{r}
kwic(data_corpus_inaugural, "division")
```

Search for the word "division" and save the results of this keywords-in-context object to a new object called `division_kwic`. Plot an "x-ray" plot for this kwic using `textplot_xray()` in the code bloc below:

```{r fig.width = 10}
division_kwic <- kwic(data_corpus_inaugural, pattern = "division")
textplot_xray(division_kwic)
```

### Question 7 (2 points)

Examine the context of words *related to* "disaster". Hint: you can use the stem of the word along with setting the `valuetype` argument to `"regex"`. Execute a query using a pattern match that returns different variations of words based on "disaster" (such as disasters, disastrous, disastrously, etc.). There is an excellent tutorial on regular expressions at <http://www.regular-expressions.info>.

```{r}
# Other regular expression are possible here
kwic(data_corpus_inaugural, pattern = "disast[A-Za-z]*", valuetype = "regex")
```

