---
title: 'Classification with Naive Bayes Solutions'
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##"
)

library("quanteda")
library("quanteda.textmodels")
```

#### Classifying movie reviews

In this assignment, we will use R to understand and apply document classification and R and quanteda. We will start with a classic computer science dataset of movie reviews, [(Pang and Lee 2004)](http://www.cs.cornell.edu/home/llee/papers/cutsent.pdf).

The movies corpus has an attribute `Sentiment` that labels each text as either `pos` or `neg` according to the original imdb.com archived newspaper review star rating.  We will begin by examining the conditional probabilities at the word level.

Let's start by loading the movies dataset and looking at the attributes:

```{r}
data(data_corpus_moviereviews, package = "quanteda.textmodels")
summary(data_corpus_moviereviews, n = 10)
```    

What is the overall probability of the class `pos` in the corpus? Are the classes balanced? (Hint: Use `table()` on the document variable of `Sentiment`.) 

**Your answer here**

>They are balanced. Each class represents 50% of the dataset.

```{r}
table(data_corpus_moviereviews$sentiment)
```
Make a dfm from the corpus, grouping the documents by the `Sentiment` document variable. Also, remove words that occur less than twenty times using `dfm_trim`. Words with very low overall frequencies in a corpus of this size are unlikely to be good general predictors.

```{r}
movieDfm <- dfm(data_corpus_moviereviews, groups = "sentiment")
movieDfmTrimmed <- dfm_trim(movieDfm, min_termfreq = 20)
movieDfmTrimmed
```

Calculate the word-level likelihoods for each class, from the reduced dfm, i.e. the probability of a word given the class `pos` and `neg`.  What are the word likelihoods for `"good"` and "`great`"? What do you learn? Use `kwic()` to find out the context of `"good"`.

Hint: you don't have to compute the probabilities by hand. You should be able to obtain them using `dfm_weight`.

**Your answer here**

> The relative frequencies within class (here, the grouped documents) are the same as the word-level likelihoods. "good" turns out to be a neutral word, while "great" is more positive than negative.

```{r}
movieDfmTrimmedRel <- dfm_weight(movieDfmTrimmed, scheme="prop")
movieDfmTrimmedRel

# probability of word given class in %
movieDfmTrimmedRel[, c("good", "great")] * 100 
```
        
```{r}
head(kwic(data_corpus_moviereviews, "good")[,4:6])
```
        

Now we will use `quanteda`â€™s naive bayes `textmodel_nb()` to run a prediction on the movie reviews.

The movie corpus contains 1000 positive examples followed by 1000 negative examples. When extracting training and testing labels, we want to get a mix of positive and negative in each set, so first we need to shuffle the corpus. Hint: You can do this with the `corpus_sample()` function.

```{r}
set.seed(1234)  # use this just before the command below
moviesShuffled <- corpus_sample(data_corpus_moviereviews, size = 2000)
```

Next, make a dfm from the shuffled corpus, and make training labels. In this case, we are using 1500 training labels, and leaving the remaining 500 unlabelled to use as a test set. We will also trim the dataset to remove rare features.

```{r}
# create dfm
movieDfm_raw <- dfm(moviesShuffled)

# trim dfm to minimum term frequency of 10
movieDfm <- dfm_trim(movieDfm_raw, min_termfreq = 10)


# training class: extract sentiment of 1500 movies
train_labels <- docvars(moviesShuffled, "sentiment")
# test class: create empty values to be filled
train_labels[1501:2000] <- NA

# convert to ordered variable
trainclass <- factor(train_labels)

table(trainclass, useNA = "ifany")
```

Now, run the training and testing commands of the Naive Bayes classifier, and compare the predictions for the documents with the actual document labels for the test set using a confusion matrix.

```{r}
movieNb <- textmodel_nb(movieDfm, trainclass)
movPreds <- predict(movieNb, newdata = movieDfm[1501:2000,])
(movTable <- table(movPreds, docvars(moviesShuffled, "sentiment")[1501:2000]))
```

### Compute model statistics

Extract the probability of a word given the class and the class prior probability from the estimated Naive Bayes model. 

Also, predict the probabilities of documents belonging to a class using `predict()`. Finally, sort the documents into classes using `predict()`. Display the first six predictions using `head()`.

```{r}
# probability of word given class
PwGc <- movieNb$param
names(dimnames(PwGc))[1] <- "classes"

# class prior probability
Pc <- movieNb$priors

# probabilities
pred_prob <- predict(movieNb, type="prob")
head(pred_prob)

# classes/categories
pred_class <- predict(movieNb, type="class")
head(pred_class)
```

Extract the posterior class probabilities of the words `good` and `great`. Do the results confirm your previous finding? Clue: look at the documentation for `textmodel_nb()` for how to extract the posterior class probabilities.

```{r}
# probability of class given word
PcGw <- PwGc * base::outer(Pc, rep(1, ncol(PwGc)))
PcGw <- matrix(sapply(PcGw, function(x) sqrt(sum(x^2))), nrow=2, dimnames = dimnames(PwGc))
names(dimnames(PcGw))[1] <- "classes"

# probability of class given word in %
PcGw[,c("good", "great")] * 100
```




Compute the following statistics for the last classification: 
    
Use this code for starters, and note that it returns something that you can use to compute \(F1\).

```{r}
precrecall <- function(mytable, verbose = T) {
    truePositives <- mytable[1,1]
    falsePositives <- sum(mytable[1,]) - truePositives
    falseNegatives <- sum(mytable[,1]) - truePositives
    precision <- truePositives / (truePositives + falsePositives)
    recall <- truePositives / (truePositives + falseNegatives)
    if (verbose) {
        print(mytable)
        cat("\n precision =", round(precision, 2), 
            "\n    recall =", round(recall, 2), "\n")
    }
    invisible(c(precision, recall))
}
```
    
Hint: Computing precision and recall is not the same if we are considering the "true positive" to be predicting positive for a true positive, versus predicting negative for a true negative.  Since the factors of `Sentiment` are ordered alphabetically, and since the table command puts lower integer codes for factors first, `movtable` by default puts the (1,1) cell as the case of predicting negative reviews as the "true positive", not predicting positive reviews.  To get the positive-postive prediction you will need to reverse index it, e.g. `movTable[2:1, 2:1]`.

1. precision and recall, *for the positive category prediction*;
        
```{r}
pr <- precrecall(movTable[2:1, 2:1])
```

2.  \(F1\) from the above; and
        
```{r}
2 * prod(pr) / sum(pr)
```

3. accuracy.
        
```{r}
sum(diag(movTable)) / sum(movTable)
```

