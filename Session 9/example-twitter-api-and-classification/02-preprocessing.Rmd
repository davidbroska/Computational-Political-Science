---
title: "Pre-processing tweets"
output: html_document
---

In this notebook, we are going to process the downloaded Twitter data in order to generate a training dataset for our classifiers.

Loading packages:

```{r}
library("tm")
library("tidyverse")
library("rtweet")
library("quanteda")
# Function which allows to recover corrupted tweet jsons returned by rtweet
# Discussion: https://github.com/ropensci/rtweet/issues/356
source("https://gist.githubusercontent.com/JBGruber/dee4c44e7d38d537426f57ba1e4f84ab/raw/ce28d3e8115f9272db867158794bc710e8e28ee5/recover_stream.R")
```

Emojis of interest in the example we consider:

```{r}
emojis_class_d <- c("\U0001F44E","\U0001F44E\U0001F3FB", "\U0001F44E\U0001F3FC", "\U0001F44E\U0001F3FD", "\U0001F44E\U0001F3FE", "\U0001F44E\U0001F3FF")
emojis_class_a <- c("\U0001F44D","\U0001F44D\U0001F3FB", "\U0001F44D\U0001F3FC", "\U0001F44D\U0001F3FD", "\U0001F44D\U0001F3FE", "\U0001F44D\U0001F3FF", "\U0001F44F", "\U0001F44F\U0001F3FB", "\U0001F44F\U0001F3FC", "\U0001F44F\U0001F3FD", "\U0001F44F\U0001F3FE", "\U0001F44F\U0001F3FF")
label_n = "neutral"
label_d = "disapprove"
label_a = "approve"
cat(emojis_class_d)
cat("\n")
cat(emojis_class_a)
```

Next, let us write a function to load all tweet json file and concatenate them into one dataframe. A current issue of the `rtweet` package is that the `stream_tweets` function can yield corrupted json files, which then are impossible to read into R afterwards. We will use the function sourced above that was written by a user and is available on GitHub. We will add a case to our function, which executes this function as an alternative whenever an error occurs. Such routing in case of errors can be very helpful, should it be of interest to your, have a look at this [book chapter](http://adv-r.had.co.nz/Exceptions-Debugging.html) by Hadley Wickham.

```{r}
load_tweets <- function(file) {
  
  # Read in the file with the standard `parse_stream` function, but switch
  # to `recover_stream` if an error occurs
  df <- tryCatch({
      
      parse_stream(file)
      
  }, error = function(e) {print(paste("Retrying with alternative function after initial error when parsing file", file)); return(recover_stream(file))})
    
  return(df)
  
}
```

Loading all files (these are files I have downloaded via the streaming API before, change accordingly for your application). For large files, this will take considerable time. Note: If an error occurs, we will need to supply some user input in the console.

```{r, eval = FALSE}
disapproval_tweets <- load_tweets("../../data/some_disapproval_emoji_tweets.json")
approval_tweets <- load_tweets("../../data/some_approval_emoji_tweets.json")

df_raw <- bind_rows(approval_tweets, disapproval_tweets)
```

Count the number of observations to start with:

```{r}
nrow(df_raw)
```

Keeping only English tweets and determining tweets labels:

```{r}
# Select only text and language column
df <- df_raw %>% select(text, lang)
# Remove all observations that are not English (note that the rtweet filter while streaming tweets seems to have worked work imperfectly here)
df <- df[df$lang == "en",]
df$lang <- NULL
# Add counts of the individual classes
df$n_emojis_class_d <- str_count(df$text, pattern = paste(emojis_class_d, collapse="|"))
df$n_emojis_class_a <- str_count(df$text, pattern = paste(emojis_class_a, collapse="|"))
# Add labels
df <- df %>% add_column(label = "unknown")
df$label[(df$n_emojis_class_d == 0) & (df$n_emojis_class_a == 0)] <- label_n
df$label[(df$n_emojis_class_d > 0) & (df$n_emojis_class_a == 0)] <- label_d
df$label[(df$n_emojis_class_d == 0) & (df$n_emojis_class_a > 0)] <- label_a
# Keep only the clearly labeled observations
df <- df[df$label %in% c(label_n, label_d, label_a),]
```

Processing the text:

```{r}
# Convert to lower case
df$text <- df$text %>% tolower()
# Remove Twitter handles and hashtags
df$text <- str_replace_all(df$text, pattern = "[@#]\\S+", replacement = "")
# Remove URLs
df$text <- str_replace_all(df$text, pattern = "(http|www)\\S+", replacement = "")
# Get rid of non ASCII characters (largely emojis in this case)
df$text <-gsub("[^\x01-\x7F]", "", df$text)
# Remove other internet garbage
df$text <- str_replace_all(df$text, pattern = "amp", replacement = "")
# Remove punctuation, numbers, and excess white spaces within the texts and at their beginning/end
df$text <- df$text %>% removePunctuation() %>%
    removeNumbers() %>% stripWhitespace() %>% trimws()
```

Removing duplicates:

```{r}
df <- df %>% distinct(text, .keep_all = TRUE)
#df <- df[sapply(strsplit(df$text, " "), length) > 1,]
```

Next we will briefly display the most common non stop words. This is a good way to see whether some residual HTML code or very frequent terms e.g. are in the corpus which are not words:

```{r}
crosscheck_dfm <- df$text %>% tokens() %>% tokens_remove(stopwords("en")) %>% dfm()
sort(colSums(crosscheck_dfm), decreasing = TRUE)[1:50]
```
Keep only those observations with at least one feature *after* removing stopwords:

```{r}
df <- df[rowSums(crosscheck_dfm) > 0, ]
```


Checking how many observation of each class are left:

```{r}
obs_counts <- df %>% group_by(label) %>% summarise(n = n())
n_label_n <- obs_counts %>% filter(label == label_n) %>% select(n) %>% as.numeric()
n_label_d <- obs_counts %>% filter(label == label_d) %>% select(n) %>% as.numeric()
n_label_a <- obs_counts %>% filter(label == label_a) %>% select(n) %>% as.numeric()
obs_counts
```

Sampling classes to an equal amount of observations:

```{r}
set.seed(123)
# Determine smallest class
n_smallest_class <- min(c(n_label_n, n_label_d, n_label_a))
# Create equal samples
df_0 <- sample_n(df %>% filter(label == label_n), n_smallest_class, replace = FALSE)
df_1 <- sample_n(df %>% filter(label == label_d), n_smallest_class, replace = FALSE)
df_2 <- sample_n(df %>% filter(label == label_a), n_smallest_class, replace = FALSE)
# Combine into one dataframe
df_downsampled <- bind_rows(list(df_0, df_1, df_2))
# Shuffle final df
df_downsampled <- sample_n(df_downsampled, nrow(df_downsampled), replace = FALSE)
```

Observations left:

```{r}
nrow(df_downsampled)
df_downsampled %>% group_by(label) %>% summarise(n = n())
```

Writing the final dataframe to disk:

```{r}
write_csv(df_downsampled, "../../data/labeled_tweets_processed.csv")
```